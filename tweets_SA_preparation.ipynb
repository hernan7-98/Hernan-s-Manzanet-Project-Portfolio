{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f752311b",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Hernan\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6476375",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8173ac-c67c-41c7-b81f-08cce1e3c20d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cbc5b48a65ef0e48bc278488914360b",
     "grade": false,
     "grade_id": "cell-a22382bdb8e4ba99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "\n",
    "# DS320 Spring 2023: Midterm Project\n",
    "\n",
    "Due on Mon 04/03/23 at 8:00 AM\n",
    "\n",
    "The focus of this assignment is data <i><b>cleaning, preparation, and basis calculations</i></b>: deal with numbers, datetime values, and text\n",
    "\n",
    "You should review the pipeline of cleaning and preparing data I drew on the whiteboard and the \"data cleaning\" slide (try to be clear about 05 steps there) before working on this assignment.\n",
    "\n",
    "You will clean tweets taken from https://github.com/thuydt02/HCQ for a text sentiment analysis problem.\n",
    "You need to download the dataset from this link, upzip it and work on the `Full_Tweet_to_github.csv` file.\n",
    "Do not upload the dataset to your Jupyter Luther since it is big. You need to download this notebook and work on it with Google Colab. I am trying to find out a way to upload the dataset to Jupyter Luther. If it is successful (likely), I will let you know. But be prepared to work in Google Colab.  \n",
    "\n",
    "The dataset has roughly 164K tweets. These tweets were pulled from Twitter, satisfies:\n",
    "\n",
    "1. created in the year 2020\n",
    "2. has the key word \"Hydroxychroloquine\"\n",
    "\n",
    "I use this dataset for my research. I want to analyze the reactions and opinions of social network users on using the medication \"Hydroxychloroqine\" to treat COVID-19 disease.\n",
    "\n",
    "See more about the paper: https://arxiv.org/pdf/2201.00237.pdf\n",
    "\n",
    "There are 10 tasks, each is worth 10 points.\n",
    "\n",
    "Note: I will mannually grade your code, so no test cases will be provided, but I can give you the expectation of the outcomes for tasks as I can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad12b9a-4724-4094-aee8-a50db05d5dca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fca787024d06eb853523e97494e91ac7",
     "grade": false,
     "grade_id": "cell-67cb81fd0b830d22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/LC/manzhe01/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#setting up: You have to run this code cell first to compile helping functions.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#functions for pre processing \n",
    "\n",
    "#---clean up html elements and entities: e.g. <html> </html> &nbsp;\n",
    "def cleanHtml(sentence):\n",
    "    #cleanr = re.compile('<.*?>')\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "#---function to clean the word of any punctuation or special characters\n",
    "def cleanPunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r' ',sentence)\n",
    "    cleaned = re.sub(r'[.|,|:|;|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "#---stop words removing \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['  ', 'zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "\n",
    "#--- sentence stemering\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3d80a-fc80-4526-9bd3-e7884d9b5e87",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "def834572886cc9a03713e98939af8e6",
     "grade": false,
     "grade_id": "cell-1a1898bff464600a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 01: Understand the data set\n",
    "- Read the whole tweet dataset (Full_Tweet_to_github.csv file) into a dataframe. Name it `df_tweets`.\n",
    "From now, you will work on `df_tweets`.\n",
    "- Find out information about the follows. You must show your code:\n",
    "    Shape\n",
    "\n",
    "    Data types of all columns\n",
    "\n",
    "    Numerical columns\n",
    "\n",
    "    Text columns\n",
    "\n",
    "    Categorical columns\n",
    "\n",
    "    Date/time columns\n",
    "\n",
    "    Statistics (min, max, mean, std, ...) of all the numerical columns\n",
    "    \n",
    " Expectation: df_tweets.shape = (164168, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2e1521-0fac-4375-b40d-0867cf639553",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "090c4371e5b4a683bfeb0ed02d6d1b23",
     "grade": true,
     "grade_id": "cell-013181b5001bcc34",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164168, 12)\n"
     ]
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('Full_Tweet_to_github.csv')\n",
    "print(df_tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896405f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70c640af4f6172e7a49997ce37235e6d",
     "grade": false,
     "grade_id": "cell-073f5375cf68d705",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 02: Delete uneccessary columns\n",
    "As you can see, the following columns will not help a machine learning algorithm to learn:\n",
    "+ HYDROXYCHLOROQUINE: all the values in this columns are 1\n",
    "+ query_string: the URL link to the tweet in Twitter.\n",
    "\n",
    "Delete two above columns from `df_tweets`\n",
    "\n",
    "expectaion: the shape of the returned dataframe = (164168, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2638f6a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83f34f82cd3d07f6feec9cbf964160e4",
     "grade": true,
     "grade_id": "cell-bf8152b9e3074cb2",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164168, 10)\n"
     ]
    }
   ],
   "source": [
    "df_tweets.drop(['HYDROXYCHLOROQUINE', 'query_string'], axis=1, inplace=True)\n",
    "print(df_tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ed468",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9ee119d4c113fab8f3ee267c0658f71",
     "grade": false,
     "grade_id": "cell-c6e43af6ea21e74c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 03: Delete columns with missing values.\n",
    "\n",
    "Delete all columns with more than 80% missing values.\n",
    "\n",
    "In the previous assignment, you were asked to create a function, `delete_cols()`, call that function on `df_tweets` and `80`.\n",
    "\n",
    "Expectation: df_tweets.shape = (164168, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca94bd8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a278fde5782af60c7936b10f10e2eac",
     "grade": true,
     "grade_id": "cell-ddd75c64811f4103",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164168, 9)\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "def delete_cols(df, threshold):\n",
    " \n",
    "    missing_pct = df.isna().sum() / len(df)\n",
    "    \n",
    "    to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "    \n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "df_tweets = delete_cols(df_tweets, 0.8)\n",
    "print(df_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9465d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66f2114d680d12b10b0655c3feaf8c40",
     "grade": false,
     "grade_id": "cell-dc1c04354414cf7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 04: Remove duplicates and irrelevant data\n",
    "\n",
    "    a. Identify duplicates: \n",
    "        \n",
    "        Two tweets are indentical if they have same \n",
    "        full_text, created_at, reply_count, favorites_count\n",
    "        \n",
    "        Two different tweets can have the same full_text as other people can re-tweet\n",
    "        \n",
    "        Note: trying to combine the above features may be expensive\n",
    "    \n",
    "    b. remove duplicates\n",
    "\n",
    "Expectation: 1277 duplicates are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d0bba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aca59b2eaa511200e839f2e90bc42c55",
     "grade": true,
     "grade_id": "cell-a3336f0940aedd7b",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41371580-2bb0-4126-a253-ea8ac458e06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 1277\n"
     ]
    }
   ],
   "source": [
    "duplicates = df_tweets.duplicated(subset=['full_text', 'created_at', 'reply_count', 'favorite_count'])\n",
    "print(f\"Number of duplicates: {duplicates.sum()}\")\n",
    "df_tweets = df_tweets[~duplicates].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8ee99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19379847411580b0723eb88a95bb4353",
     "grade": false,
     "grade_id": "cell-71879c841157d7b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 05: Clean text\n",
    "\n",
    "Do the following sub tasks for all values in each text columns:\n",
    "+ lowcase\n",
    "+ strip spaces (leading and trailing)\n",
    "\n",
    "Call the functions you have done in the tasks 06 and 07 of the previous assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f73707",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2f249e456828f4e0b88e4a7ee24699c",
     "grade": true,
     "grade_id": "cell-df51ed24786bb704",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                full_text  created_at  \\\n",
      "0       from the january 2020 arthritis care &amp; res...  2020-01-03   \n",
      "1       @thelancetrheum classic “bulls eye retinopathy...  2020-01-07   \n",
      "2       dr. anil pareek -research has revealed that hy...  2020-01-12   \n",
      "3       a randomised phase ii trial of hydroxychloroqu...  2020-01-13   \n",
      "4       @geekyjules what are the side effects of the b...  2020-01-14   \n",
      "...                                                   ...         ...   \n",
      "162886  @ritatay29333372 @edhunter54 @specialistgi @je...  2020-12-30   \n",
      "162887  @alienxperience @sarahksilverman @marcorubio i...  2020-12-30   \n",
      "162888  @hcphtx @sonyiego @abc13houston @fox26houston ...  2020-12-30   \n",
      "162889  @drtomfrieden the covid-19 response is medical...  2020-12-30   \n",
      "162890  @drtomfrieden * reduce hospitalizations *\\nicu...  2020-12-30   \n",
      "\n",
      "           user_location  friends_count  followers_count  reply_count  \\\n",
      "0                    NaN              5             4413            0   \n",
      "1                    NaN             20               10            0   \n",
      "2       hyderabad, india          25050            32355            0   \n",
      "3                    NaN             33               16            0   \n",
      "4          colorado, usa           2619             1075            2   \n",
      "...                  ...            ...              ...          ...   \n",
      "162886               NaN            102               12            1   \n",
      "162887               NaN           1645            12921            1   \n",
      "162888                ca            262              156            0   \n",
      "162889                ca            262              156            0   \n",
      "162890                ca            262              156            0   \n",
      "\n",
      "        retweet_count  favorite_count  is_with_url  \n",
      "0                   2              10            1  \n",
      "1                   0               1            0  \n",
      "2                   0               0            1  \n",
      "3                   0               0            1  \n",
      "4                   0               0            0  \n",
      "...               ...             ...          ...  \n",
      "162886              0               1            0  \n",
      "162887              0               1            0  \n",
      "162888              0               0            1  \n",
      "162889              0               1            1  \n",
      "162890              0               0            1  \n",
      "\n",
      "[162891 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for column in df_tweets.select_dtypes(include='object'):\n",
    "    df_tweets[column] = df_tweets[column].str.lower()\n",
    "    # Apply strip spaces transformation\n",
    "    df_tweets[column] = df_tweets[column].str.strip()\n",
    "\n",
    "print(df_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc6acd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e63358a579ee1d976cc491fc6c2d4ff2",
     "grade": false,
     "grade_id": "cell-5915a3db46ac7d43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 06: Clean tweets\n",
    "\n",
    "Do the following sub tasks for all values in the `full_text` column:\n",
    "+ only keep alphabetical letters\n",
    "+ remove HTML tags and entities\n",
    "+ remove punctuations\n",
    "+ remove stop words (words have no contribution for sentiment identification of sentences)\n",
    "+ stemming words: replace a word with its original version since they have the same meaning and sentiment in a sentence. For example, `happiness` is derived from `happy` => we will replace `happiness` by `happy`\n",
    "\n",
    "You are provided all the functions in the setting up cell. Call them for this task.\n",
    "\n",
    "Note this task will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a28128",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7fbcec1035621eb446ad8a10894964d",
     "grade": true,
     "grade_id": "cell-c2b55833b91ef73d",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(cleanHtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c67dc14-25c4-4f15-b568-8c14ac462fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r' ',sentence)\n",
    "    cleaned = re.sub(r'[.|,|:|;|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(cleanPunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f29d7d6f-bbd6-4038-861f-fd308b075df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40374fd8-6ee0-48e3-a141-95bd6e22f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf5ff3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19ba779aa279002f2329f82d6ad2dc26",
     "grade": false,
     "grade_id": "cell-d2c710329ea735f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 07: Add a new column\n",
    "\n",
    "Add a new column, called `state` and fill values in this column using the the following instructions.\n",
    "\n",
    "We want to derive a state from `user_location`, but the data in `user_location` is very messy: some have cities' names (e.g., Albany), some have cities' names and state, ...\n",
    "\n",
    "So I am including a look-up table file, called `state_full.csv`, here for you. In this file I have 2 columns: `shortState` and `city`. Whenever an user location has a short state name or a city in columns `shortState` or `city`, you will fill the `state` column with the short state.\n",
    "\n",
    "For examples:\n",
    "\n",
    "`user_location` = 'albany, usa' => `state` = 'NY'\n",
    "\n",
    "`user_location` = 'boston, massachusetts' => `state` = 'MA'\n",
    "\n",
    "`user_location` = 'ames, ia' => `state` = 'IA'\n",
    "\n",
    "`user_location` = 'boston' => `state` = 'MA'\n",
    "\n",
    "...\n",
    "Note: this task will take long time (about 3-4 hours) to run\n",
    "\n",
    "Expectation: 53712 short state names (NaN values are not counted) will be filled \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "355b1469",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c73ad51d3390c995930c6c891f2c9e82",
     "grade": true,
     "grade_id": "cell-88f3c6ec8ed348f2",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      66603\n",
      "CA    17616\n",
      "IN    11299\n",
      "OR     6952\n",
      "IL     4971\n",
      "TX     4376\n",
      "NY     4338\n",
      "MA     4299\n",
      "FL     3745\n",
      "NE     3622\n",
      "PA     3430\n",
      "LA     3401\n",
      "CO     3285\n",
      "MI     2432\n",
      "AR     2375\n",
      "WA     2254\n",
      "AL     2147\n",
      "NC     1949\n",
      "OH     1556\n",
      "VA     1439\n",
      "MO     1277\n",
      "GA     1060\n",
      "AZ      987\n",
      "ND      880\n",
      "WI      591\n",
      "UT      584\n",
      "IA      557\n",
      "NV      404\n",
      "MD      379\n",
      "KY      371\n",
      "HI      367\n",
      "ID      359\n",
      "ME      351\n",
      "NJ      340\n",
      "OK      310\n",
      "RI      296\n",
      "DE      263\n",
      "KS      238\n",
      "TN      219\n",
      "AK      164\n",
      "NM      157\n",
      "SC      141\n",
      "MN      130\n",
      "DC      113\n",
      "MS       98\n",
      "CT       75\n",
      "NH       57\n",
      "MT       12\n",
      "SD        9\n",
      "WV        8\n",
      "WY        3\n",
      "VT        2\n",
      "Name: state, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "state_df = pd.read_csv('state_full.csv')\n",
    "def getState(location):\n",
    "    if isinstance(location, float):\n",
    "        return ''\n",
    "    for state in state_df['shortState']:\n",
    "        if state.lower() in str(location).lower():\n",
    "            return state\n",
    "    for city in state_df['city']:\n",
    "        if city.lower() in str(location).lower():\n",
    "            return state_df.loc[state_df['city'].str.lower() == city.lower(), 'shortState'].values[0]\n",
    "    return ''\n",
    "\n",
    "df_tweets['state'] = df_tweets['user_location'].apply(getState)\n",
    "state_counts = df_tweets['state'].value_counts()\n",
    "\n",
    "print(state_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe1c1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91c16ddceef76f11e946f68f09c46362",
     "grade": false,
     "grade_id": "cell-059b23f932324a02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 08: count #tweets and sum up #favorite_count by date\n",
    "\n",
    "Count number of tweets and sum up favorite_count by dates. Store the results in a dataframe, called `df_count_by_date`. Sort the dataframe in the descending order of tweet counts.\n",
    "\n",
    "hint: use `groupby()` on `df_tweets` and then `agg()` with count for full_text and sum for favorite_count\n",
    "\n",
    "expectation: df_count_by_date.shape = (315, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15e4d2a1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6accf813ab3a7ffe0b0daeeedd00114",
     "grade": true,
     "grade_id": "cell-abd5d6c440c00379",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315, 2)\n"
     ]
    }
   ],
   "source": [
    "df_tweets['created_at'] = pd.to_datetime(df_tweets['created_at'])\n",
    "\n",
    "df_count_by_date = df_tweets.groupby(df_tweets['created_at'].dt.date).agg({'full_text': 'count', 'favorite_count': 'sum'})\n",
    "\n",
    "df_count_by_date = df_count_by_date.sort_values(by='full_text', ascending=False)\n",
    "\n",
    "print(df_count_by_date.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83face4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7256b2b2c36bce048a0704fed0ea739b",
     "grade": false,
     "grade_id": "cell-bf4ad2ae2956c051",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 09: count #tweets and sum up #favorite_count by state\n",
    "\n",
    "Count number of tweets and sum up favorite_count by states. Store the results in a dataframe, called `df_count_by_state`. Sort the dataframe in the descending order of tweet counts.\n",
    "\n",
    "hint: use `groupby()` on `df_tweets` and then `agg()` with count for full_text and sum for favorite_count\n",
    "\n",
    "expectation: df_count_by_state.shape = (51, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b6fe902",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "851314115106ce916eaf2427c100604a",
     "grade": true,
     "grade_id": "cell-69d7ce514f863772",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 2)\n"
     ]
    }
   ],
   "source": [
    "df_count_by_state = df_tweets.groupby('state').agg({'full_text': 'count', 'favorite_count': 'sum'})\n",
    "\n",
    "# Sort the DataFrame by tweet counts in descending order\n",
    "df_count_by_state = df_count_by_state.sort_values(by='full_text', ascending=False)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_count_by_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e252e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "814ac3187e6f19ed6dc5576389aea12e",
     "grade": false,
     "grade_id": "cell-52d576ec65ad099d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 10: Top 10 tweets\n",
    "\n",
    "+ Find tweets in top 10 highest reply_count, ordering from the highest to the least.\n",
    "+ Find tweets in top 10 highest retweet_count, ordering from the highest to the least.\n",
    "+ Find tweets in top 10 highest favorite_count, ordering from the highest to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dafa1cff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "419d768f2d078c5b26eace921fe407ce",
     "grade": true,
     "grade_id": "cell-703d87775eb8aeb4",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               full_text  created_at  \\\n",
      "484    hydroxychloroquine azithromycin taken together...  2020-03-21   \n",
      "9323               hydroxychloroquine httpstcoymobdcfgxs  2020-05-19   \n",
      "93441  trump kept telling us take hydroxychloroquine ...  2020-10-06   \n",
      "13294  highly respected henry ford health system repo...  2020-07-07   \n",
      "52696  trump receiving regenerons polyclonal antibody...  2020-10-02   \n",
      "486    please dont take hydroxychloroquine plaquenil ...  2020-03-21   \n",
      "90726  president receiving multiple medications notew...  2020-10-05   \n",
      "70263                hydroxychloroquine stand back stand  2020-10-02   \n",
      "16840  please watch highly respected dr harvey risch ...  2020-08-24   \n",
      "14600  imagine child porn taken social media quickly ...  2020-07-29   \n",
      "\n",
      "                     user_location  friends_count  followers_count  \\\n",
      "484                 washington, dc             50         85725414   \n",
      "9323                  brooklyn, ny           3005          2276258   \n",
      "93441          eugene@coolquit.com           6154           524500   \n",
      "13294               washington, dc             50         85725414   \n",
      "52696          eugene@coolquit.com           6154           524500   \n",
      "486    republic of the philippines            335            27605   \n",
      "90726                          NaN            870          1397401   \n",
      "70263      los angeles, california           6315           115264   \n",
      "16840               washington, dc             50         85725414   \n",
      "14600              los angeles, ca            327            11610   \n",
      "\n",
      "       reply_count  retweet_count  favorite_count  is_with_url  \n",
      "484          69341         101604          374415            0  \n",
      "9323          5664          50913          206341            0  \n",
      "93441         2558          63176          203950            0  \n",
      "13294        24714          48914          163587            0  \n",
      "52696         1574          37584          153426            0  \n",
      "486           2501          69684          139878            0  \n",
      "90726         2228          22678          135147            0  \n",
      "70263          855          18917          126349            0  \n",
      "16840        15948          53778          124556            0  \n",
      "14600         1997          38429          123763            0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_10_favorite_count = df_tweets.sort_values(by='favorite_count', ascending=False).head(10)\n",
    "print(top_10_favorite_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faacfe90-fac1-43d8-8c40-cbd2980fec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               full_text  created_at  \\\n",
      "484    hydroxychloroquine azithromycin taken together...  2020-03-21   \n",
      "486    please dont take hydroxychloroquine plaquenil ...  2020-03-21   \n",
      "93441  trump kept telling us take hydroxychloroquine ...  2020-10-06   \n",
      "540    please spread hydroxychloroquine azithromycin ...  2020-03-21   \n",
      "16840  please watch highly respected dr harvey risch ...  2020-08-24   \n",
      "9323               hydroxychloroquine httpstcoymobdcfgxs  2020-05-19   \n",
      "13294  highly respected henry ford health system repo...  2020-07-07   \n",
      "12905  want ensure everyone understands gravity situa...  2020-07-03   \n",
      "13030  hydroxychloroquine works worked whole time tha...  2020-07-04   \n",
      "14600  imagine child porn taken social media quickly ...  2020-07-29   \n",
      "\n",
      "                     user_location  friends_count  followers_count  \\\n",
      "484                 washington, dc             50         85725414   \n",
      "486    republic of the philippines            335            27605   \n",
      "93441          eugene@coolquit.com           6154           524500   \n",
      "540                   new york, ny             48             6703   \n",
      "16840               washington, dc             50         85725414   \n",
      "9323                  brooklyn, ny           3005          2276258   \n",
      "13294               washington, dc             50         85725414   \n",
      "12905          manhattan, new york           4636           321973   \n",
      "13030          manhattan, new york           4636           321973   \n",
      "14600              los angeles, ca            327            11610   \n",
      "\n",
      "       reply_count  retweet_count  favorite_count  is_with_url  \n",
      "484          69341         101604          374415            0  \n",
      "486           2501          69684          139878            0  \n",
      "93441         2558          63176          203950            0  \n",
      "540           1433          53780           89525            0  \n",
      "16840        15948          53778          124556            0  \n",
      "9323          5664          50913          206341            0  \n",
      "13294        24714          48914          163587            0  \n",
      "12905         2688          48780           85632            0  \n",
      "13030         1670          41374          107698            0  \n",
      "14600         1997          38429          123763            0  \n"
     ]
    }
   ],
   "source": [
    "top_10_retweet_count = df_tweets.sort_values(by='retweet_count', ascending=False).head(10)\n",
    "print(top_10_retweet_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7596887c-993d-4161-84c7-388337a702bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               full_text  created_at  \\\n",
      "484    hydroxychloroquine azithromycin taken together...  2020-03-21   \n",
      "15326  taking hydroxychloroquine treat coronavirus di...  2020-07-31   \n",
      "12924  surprising new study found controversial antim...  2020-07-03   \n",
      "13294  highly respected henry ford health system repo...  2020-07-07   \n",
      "3531   listen 45 suggests untested hydroxychloroquine...  2020-04-06   \n",
      "13399  obamas hydroxychloroquine 2008 httpstcoodnmltdcbu  2020-07-11   \n",
      "15254  actually believe hydroxychloroquine probably e...  2020-07-30   \n",
      "5495   trump says jesus could avoided crucifixion tak...  2020-04-12   \n",
      "16840  please watch highly respected dr harvey risch ...  2020-08-24   \n",
      "785    breaking man died wife icu ingested chloroquin...  2020-03-23   \n",
      "\n",
      "                      user_location  friends_count  followers_count  \\\n",
      "484                  washington, dc             50         85725414   \n",
      "15326                           NaN            745           342719   \n",
      "12924                           NaN           1106         49712635   \n",
      "13294                washington, dc             50         85725414   \n",
      "3531   los angeles/washington, d.c.            691          1468361   \n",
      "13399            underground bunker              6          2432611   \n",
      "15254                     ohio, usa           1601            68915   \n",
      "5495                    los angeles              1         28564321   \n",
      "16840                washington, dc             50         85725414   \n",
      "785                         florida            389           317113   \n",
      "\n",
      "       reply_count  retweet_count  favorite_count  is_with_url  \n",
      "484          69341         101604          374415            0  \n",
      "15326        29771          33410           69840            0  \n",
      "12924        25817          23668           38343            1  \n",
      "13294        24714          48914          163587            0  \n",
      "3531         22788          15749           59642            0  \n",
      "13399        19417          18276           35590            0  \n",
      "15254        19400           3307           21780            0  \n",
      "5495         17791          13896          108249            0  \n",
      "16840        15948          53778          124556            0  \n",
      "785          14391           8954           12417            1  \n"
     ]
    }
   ],
   "source": [
    "top_10_reply_count = df_tweets.sort_values(by='reply_count', ascending=False).head(10)\n",
    "print(top_10_reply_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadfb829-b171-4237-8588-67dc2c89fc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS320 (Python3.10)",
   "language": "python",
   "name": "ds320"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
